#  CWDset: A Remote Sensing Dataset for Semantic Segmentation of Small-Scale Coal Waste Dumps


## Key Features

* **Dataset:** 4-channel (R, G, B, NIR) high-resolution remote sensing imagery.
* **Custom Code:** Includes robust data loaders (`LoadTiffImageFromFile`,`LoadTiffAnnotations`) and augmentation pipelines (`RandomCropTiff`, `RandomFlipTiff`,`RandomResizeTiff`) **specifically designed for 4-channel TIF data**.
* **Benchmarks:** "Plug-and-play" configurations for 7 major segmentation models (BiSeNetV2, DeepLabV3+, Mask2Former, OCRNet, PSPNet, SegFormer, UNet).
* **Environment:** A 100% locked, reproducible, and stable Conda + PyTorch environment.

---

## ğŸ”§ 1. Installation
### Step 1: Download and Unzip Repository
1. **Download ZIP:** Click the green **< > Code** button, and then select **Download ZIP**.
2. **Rename and Prepare:** Unzip the downloaded file. 
### Step 2: Create the Conda Environment
```bash
# 1. Create a clean Python 3.10 environment
conda create -n mmlab_stable python=3.10 -y

# 2. Activate the new environment
conda activate mmlab_stable
```
### Step 3ï¼šInstall PyTorch and mmcv-full
```bash
# 1. Install PyTorch 2.1.2 (LTS) + CUDA 12.1 Toolkit
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121

# 2. Install "mmcv-full" (v2.1.0)
pip install "mmcv==2.1.0" -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.1/index.html
```
### Step 4:Install All Remaining Dependencies
```bash
pip install -r requirements.txt
```
## 2. Data Preparation

1ã€Due to the large file size, the CWDset is currently hosted on Google Drive (Official DOI will be provided upon publication):
    https://drive.google.com/file/d/1bYEgSKtDWlKEjYFPf4ZDUr1bgHC-cAaB/view?usp=sharing

2ã€Download the dataset.

3ã€Unzip the file and ensure your final project structure looks like this: 
```bash
pythonProject/
â”œâ”€â”€ configs/
â”œâ”€â”€ CWDset_code/
â”œâ”€â”€ tools/
â”œâ”€â”€ work_dirs/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ CWDset/
â”‚       â”œâ”€â”€ images/
â”‚       â”‚   â”œâ”€â”€ train/
â”‚       â”‚   â”‚   â”œâ”€â”€ img1.tif
â”‚       â”‚   â”‚   â””â”€â”€ ...
â”‚       â”‚   â””â”€â”€ val/
â”‚       â”‚       â””â”€â”€ ...
â”‚       â””â”€â”€ labels/
â”‚           â”œâ”€â”€ train/
â”‚           â”‚   â”œâ”€â”€ img1.tif
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ val/
â”‚               â””â”€â”€ ...
â””â”€â”€ README.md
```
## 3.How to Reproduce
train  Models 
All 7 models are configured to train for 110,000 iterations and will only save the single best checkpoint based on val/mIoU.
All models were trained in the mmlab_stable environment on a single NVIDIA RTX 4080 SUPER for 110,000 iterations.

Example: Train BiSeNetV2: 
```bash
python tools/train.py configs/bisenetv2_fcn_b4_110k_CWDset-512x512.py
```
Testing (Evaluate mIoU) 
You can evaluate mIoU using either the weights you trained yourself, or our provided pre-trained weights.

Pre-trained Weights Location: We provide the final benchmark weights for all 7 models. During the review process, these can be downloaded from:
https://drive.google.com/file/d/1o0KlEtDVnIrHqlXDKbDED3aj3OmTv6Yr/view?usp=sharing.

Please download these weights (e.g., into a new pre-trained/ folder). 

Example: Test BiSeNetV2 using our pre-trained weight: 
```bash
# Usage: python tools/test.py [CONFIG_FILE] [CHECKPOINT_FILE]
python tools/test.py configs/bisenetv2_fcn_b4_110k_CWDset-512x512.py pre-trained/bisenetv2.pth
```
Example: Test your self-trained BiSeNetV2 model: 
```bash
python tools/test.py configs/bisenetv2_fcn_b4_110k_CWDset-512x512.py work_dirs/bisenetv2_fcn_b4_110k_CWDset-512x512/best_val_mIoU_iter_XXXX.pth
```


